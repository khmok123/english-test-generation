{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83abe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang('simple')\n",
    "dt = wikipedia.page(\"Donald Trump\")\n",
    "text = dt.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "752f4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "This is the most weird day that I have ever had . I had been walking to my friend's house when I saw a bull in the street!. In that moment, I ran too fast to the opposite direcion meanwhile I was screaming for help. I had experienced a similar situation like that, but in a secure environment .\n",
    "\n",
    "I have a mix of feelings now. I am thinking about before, I have been running too fast, I thought that the bull was near me after that race.\n",
    "\n",
    "The next year I will have been writing in this diary for 10 years, and this is the most weird fact that I have ever had.\n",
    "\n",
    "Now, what I want is: talk with my friend face-to-face about that moment, so, I think that I will go to his house tomorrow well the path, because I could see other one again!)\n",
    "\n",
    "I will close this crazy day with the hope that for the next year I will not have lived another day like this.\n",
    "\n",
    "I could have tried and should have tried it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_matches(matches):\n",
    "    \n",
    "    intervals = [[match[1],match[2]] for match in matches]\n",
    "    \n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            # test for intersection between lower and higher:\n",
    "            # we know via sorting that lower[0] <= higher[0]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = [lower[0], upper_bound]  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    indices = []\n",
    "    errors = 0\n",
    "    error_merges = []\n",
    "    \n",
    "    for merge in merged:\n",
    "        try:\n",
    "            indices.append(intervals.index(merge))\n",
    "        except:\n",
    "            errors += 1\n",
    "            error_merges.append(merge)\n",
    "            \n",
    "    print(str(errors) + \"errors occured. The erroroneous merge is \" + str(error_merges))\n",
    "    \n",
    "    new_matches = [matches[index] for index in indices]\n",
    "    \n",
    "    return new_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b48d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "Tenses: \n",
    "Present: VBP/VBZ\n",
    "Present continuous: am/is/are/'m/'s/'re + (RB) + VG\n",
    "Past: VBD\n",
    "Past continuous: was/were + (RB) + VG\n",
    "Future: will/wo/shall + (RB) + VB\n",
    "Future II: am/is/are/'m/'s/'re + (RB) + going + to + VB\n",
    "Future continuous: will/wo/'ll + (RB) + be + (RB) + VG\n",
    "Present perfect: have/has/'ve' + (RB) + VBN\n",
    "Present perfect continuous: have/has + (RB) + been + (RB) + VG\n",
    "Past perfect: had + (RB) + VBN\n",
    "Past perfect continuous: had + (RB) + been + (RB) + VG\n",
    "Future perfect: will/wo/'ll + (RB) + have + (RB) + VBN\n",
    "Future perfect continuous: will/wo + (RB) + been + (RB) + VG\n",
    "Modal verbs: can/could/may/might/should/must/would/'d + (RB) + VB + (RB) + V(any)\n",
    "(Actually include future tenses)\n",
    "(*need to handle tenses myself)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6f36371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matches:\n",
      "present_tense 2 3 is\n",
      "present_perfect 9 12 have ever had\n",
      "present_tense 9 11 have ever\n",
      "past_perfect 14 16 had been\n",
      "past_perfect_continuous 14 17 had been walking\n",
      "past_tense 14 15 had\n",
      "past_tense 24 25 saw\n",
      "past_tense 37 38 ran\n",
      "past_continuous 46 48 was screaming\n",
      "past_tense 46 47 was\n",
      "past_perfect 52 54 had experienced\n",
      "past_tense 52 53 had\n",
      "present_tense 68 69 have\n",
      "present_continuous 76 78 am thinking\n",
      "present_tense 76 77 am\n",
      "present_perfect 82 84 have been\n",
      "present_perfect_continuous 82 85 have been running\n",
      "present_tense 82 83 have\n",
      "past_tense 89 90 thought\n",
      "past_tense 93 94 was\n",
      "future_tense 105 107 will have\n",
      "future_perfect 105 108 will have been\n",
      "future_perfect_continuous 105 109 will have been writing\n",
      "present_perfect 106 108 have been\n",
      "present_perfect_continuous 106 109 have been writing\n",
      "present_tense 118 119 is\n",
      "present_perfect 125 128 have ever had\n",
      "present_tense 125 127 have ever\n",
      "present_tense 134 135 want\n",
      "present_tense 135 136 is\n",
      "present_tense 153 154 think\n",
      "future_tense 156 158 will go\n",
      "future_tense 177 179 will close\n",
      "future_tense 191 194 will not have\n",
      "future_perfect 191 195 will not have lived\n",
      "present_perfect 193 195 have lived\n",
      "present_perfect 203 205 have tried\n",
      "present_perfect 207 209 have tried\n",
      "\n",
      "\n",
      "1errors occured. The erroroneous merge is [[134, 136]]\n",
      "New matches:\n",
      "present_tense 2 3 is\n",
      "present_perfect 9 12 have ever had\n",
      "past_perfect_continuous 14 17 had been walking\n",
      "past_tense 24 25 saw\n",
      "past_tense 37 38 ran\n",
      "past_continuous 46 48 was screaming\n",
      "past_perfect 52 54 had experienced\n",
      "present_tense 68 69 have\n",
      "present_continuous 76 78 am thinking\n",
      "present_perfect_continuous 82 85 have been running\n",
      "past_tense 89 90 thought\n",
      "past_tense 93 94 was\n",
      "future_perfect_continuous 105 109 will have been writing\n",
      "present_tense 118 119 is\n",
      "present_perfect 125 128 have ever had\n",
      "present_tense 153 154 think\n",
      "future_tense 156 158 will go\n",
      "future_tense 177 179 will close\n",
      "future_perfect 191 195 will not have lived\n",
      "present_perfect 203 205 have tried\n",
      "present_perfect 207 209 have tried\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "\n",
    "# Remove parentheses and the contents contained in them\n",
    "text = re.sub(r'\\([^)]*\\)', '',text)\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tenses_priority = [\"future_perfect_continuous\",\"past_perfect_continuous\",\n",
    "                  \"present_perfect_continuous\", \"future_perfect\", \"past_perfect\",\n",
    "                  \"present_perfect\", \"future_continuous\", \"past_continuous\",\n",
    "                  \"present_continuous\", \"future_tense\", \"past_tense\", \"present_tense\", '']\n",
    "\n",
    "is_list = [\"is\",\"am\",\"are\",\"\\'s\",\"\\'re\",\"ain\\'t\"]\n",
    "do_list = [\"do\", \"does\"]\n",
    "was_list = [\"was\",\"were\"]\n",
    "will_list = [\"will\",\"wo\",\"shall\"]\n",
    "have_list = [\"have\",\"\\'ve\",\"has\",\"\\'s\"]\n",
    "adv_dict = {\"TAG\":\"RB\",\"OP\":\"*\"}\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "present_tense_pattern = [[{\"TAG\":{\"IN\":[\"VBP\",\"VBZ\"]}},adv_dict,{\"TAG\":\"VB\", \"OP\":\"*\"}]]\n",
    "past_tense_pattern = [[{\"LOWER\":\"did\"},adv_dict,{\"TAG\":\"VB\"}],\n",
    "                    [{\"TAG\":\"VBD\"}],\n",
    "                     [{\"LOWER\":{\"IN\":was_list+[\"did\"]}},adv_dict,{\"TAG\":\"PRP\"},adv_dict]]\n",
    "future_tense_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,{\"TAG\":\"VB\"}]]\n",
    "present_cont_pattern = [[{\"LOWER\":{\"IN\":is_list}},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "past_cont_pattern = [[{\"LOWER\":{\"IN\":was_list}},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "future_cont_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,\n",
    "                       {\"LOWER\":\"be\"},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "present_perfect_pattern = [[{\"LOWER\":{\"IN\":have_list}},\n",
    "                            adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "past_perfect_pattern = [[{\"LOWER\":\"had\"},adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "future_perfect_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,\n",
    "                          {\"LOWER\":{\"IN\":have_list}},adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "present_perfect_cont_pattern = [[{\"LOWER\":{\"IN\":have_list}},adv_dict,\n",
    "                                 {\"LOWER\":\"been\"},adv_dict,\n",
    "                                 {\"TAG\":\"VBG\"}]]\n",
    "past_perfect_cont_pattern = [[{\"LOWER\":\"had\"},adv_dict,{\"LOWER\":\"been\"},\n",
    "                              adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "future_perfect_cont_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,{\"LOWER\":\"have\"},\n",
    "                                adv_dict,{\"LOWER\":\"been\"},adv_dict,\n",
    "                                {\"TAG\":\"VBG\"}]]\n",
    "modal_present_pattern = [[\n",
    "    \n",
    "]]\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "matcher.add(\"present_tense\", present_tense_pattern, greedy='LONGEST')\n",
    "# matcher.add(\"present_tense_question\", present_tense_q)\n",
    "matcher.add(\"past_tense\", past_tense_pattern, greedy='LONGEST')\n",
    "matcher.add(\"future_tense\", future_tense_pattern)\n",
    "matcher.add(\"present_continuous\", present_cont_pattern)\n",
    "matcher.add(\"past_continuous\", past_cont_pattern)\n",
    "matcher.add(\"future_continuous\", future_cont_pattern)\n",
    "matcher.add(\"present_perfect\", present_perfect_pattern)\n",
    "matcher.add(\"past_perfect\", past_perfect_pattern)\n",
    "matcher.add(\"future_perfect\", future_perfect_pattern)\n",
    "matcher.add(\"present_perfect_continuous\", present_perfect_cont_pattern)\n",
    "matcher.add(\"past_perfect_continuous\", past_perfect_cont_pattern)\n",
    "matcher.add(\"future_perfect_continuous\", future_perfect_cont_pattern)\n",
    "\n",
    "# replace_word(text, \"__________\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x:x[1])\n",
    "\n",
    "print(\"Original matches:\")\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)\n",
    "            \n",
    "print(\"\\n\")\n",
    "        \n",
    "matches = get_greedy_matches(matches)\n",
    "\n",
    "print(\"New matches:\")\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddcb28",
   "metadata": {},
   "source": [
    "This is the most weird day that I have ever had . I had been walking  to my friend's house when  I saw  a bull in the street!. In that moment, I ran too fast to the opposite direcion meanwhile I was screaming  for help. I had experienced  a similar situation like that, but in a secure environment .\n",
    "\n",
    "I have  a mix of feelings now. I am thinking  about before, I have been running  too fast, I thought that the bull was near me after that race.\n",
    "\n",
    "The next year I will have been writting  in this diary for 10 years, and this is the most weird fact that I have ever had.\n",
    "\n",
    "Now, what I want is: talk with my friend face-to-face about that moment, so, I think that I will go  to his house tomorrow  well the path, because I could see other one again!)\n",
    "\n",
    "I will close this crazy day with the hope that for the next year I will not have lived  another day like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1320b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [[{\"TAG\":{\"IN\":['VBZ','VBP']}},adv_dict,{\"POS\":\"VERB\",\"OP\":\"!\"}]]\n",
    "\n",
    "text = \"I have not done yet.\"\n",
    "doc = nlp(text)\n",
    "matcher2 = Matcher(nlp.vocab)\n",
    "matcher2.add(\"pattern\",pattern)\n",
    "matches = matcher2(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3e3f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      PRP       nsubj     \n",
      "Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "wo          AUX       MD        aux       \n",
      "VerbType=Mod\n",
      "n't         PART      RB        neg       \n",
      "\n",
      "do          VERB      VB        ROOT      \n",
      "VerbForm=Inf\n",
      "it          PRON      PRP       dobj      \n",
      "Case=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      ".           PUNCT     .         punct     \n",
      "PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I won't do it.\")\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_tag = token.tag_\n",
    "    token_dep = token.dep_\n",
    "    token_morph = token.morph\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_tag:<10}{token_dep:<10}\")\n",
    "    print(token_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ce0bd370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, past participle'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"VBN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
