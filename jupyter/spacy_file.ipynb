{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d00c8d",
   "metadata": {},
   "source": [
    "https://applied-language-technology.readthedocs.io/en/latest/notebooks/part_iii/02_pattern_matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83abe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "wikipedia.set_lang('simple')\n",
    "dt = wikipedia.page(\"Donald Trump\")\n",
    "text = dt.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752f4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Powerful, low-frequency sound waves could be used to trigger rainfall in areas that suffer from drought, according to a study by researchers at Tsinghua University in Beijing.\n",
    "\n",
    "In a weather manipulation experiment conducted on the Tibetan Plateau last year, the researchers, led by Professor Wang Guangqian from the university’s State Key Laboratory of Hydro-science and Engineering, said they recorded increases in rainfall of up to 17 per cent by pointing a giant loudspeaker at the sky.\n",
    "\n",
    "The sound energy might have changed cloud physics, but the cause would require further investigation, researchers said in a peer-reviewed paper published in Scientia Sinica Technologica last week.\n",
    "\n",
    "Unlike other rainmaking technologies, sound generation produced no chemical pollution and required no “airborne vehicles such as aircraft or rockets”, Wang said. “And there is the possibility of remote control with low cost.”\n",
    "\n",
    "The experiment is likely to add fuel to the long-running debate in China on the feasibility and environmental impact of large-scale weather modification programmes.\n",
    "\n",
    "Critics have accused Wang, who proposed the controversial Sky River project to increase rainfall across Tibet by intercepting wet air circulating over the plateau, of wasting taxpayers’ money. Others say that even if the sound stimulation method works, it would create noise pollution for the people and animals that live in the area.\n",
    "\n",
    "In the study, the rainfall was 11 to 17 per cent higher in areas within the device’s effective range – a radius of about 500 metres from the sound generator – than outside it.\n",
    "\n",
    "Despite the findings, a researcher from the Institute of Atmospheric Physics under the Chinese Academy of Sciences in Beijing who asked not to be named said Wang’s two-hour experiment would have to be replicated many times to gather more data.\n",
    "\n",
    "While there has long been speculation that rainfall might be linked to sound – many civilisations perform rain dances in times of drought – the person said there were no physical theories to support the idea.\n",
    "\n",
    "“The subject remains more of myth than science,” he said.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde27c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blanks:  24\n",
      "present_tense 17 18 suffer\n",
      "past_tense 69 70 said\n",
      "past_tense 71 72 recorded\n",
      "present_perfect 95 97 have changed\n",
      "past_tense 109 110 said\n",
      "past_tense 132 133 produced\n",
      "past_tense 137 138 required\n",
      "past_tense 150 151 said\n",
      "present_tense 155 156 is\n",
      "present_tense 169 170 is\n",
      "present_perfect 198 200 have accused\n",
      "past_tense 203 204 proposed\n",
      "present_tense 230 231 say\n",
      "present_tense 238 239 works\n",
      "present_tense 251 252 live\n",
      "past_tense 263 264 was\n",
      "past_tense 316 317 asked\n",
      "past_tense 321 322 said\n",
      "present_perfect 343 346 has long been\n",
      "present_tense 357 358 perform\n",
      "past_tense 367 368 said\n",
      "past_tense 369 370 were\n",
      "present_tense 382 383 remains\n",
      "past_tense 391 392 said\n",
      "Powerful, low - frequency sound waves could be used to trigger rainfall in areas that ________________ (suffer) from drought, according to a study by researchers at Tsinghua University in Beijing. \n",
      "\n",
      " In a weather manipulation experiment conducted on the Tibetan Plateau last year, the researchers, led by Professor Wang Guangqian from the university ’s State Key Laboratory of Hydro - science and Engineering, ________________ (say) they ________________ (record) increases in rainfall of up to 17 per cent by pointing a giant loudspeaker at the sky. \n",
      "\n",
      " The sound energy might ________________ (change) cloud physics, but the cause would require further investigation, researchers ________________ (say) in a peer - reviewed paper published in Scientia Sinica Technologica last week. \n",
      "\n",
      " Unlike other rainmaking technologies, sound generation ________________ (produce) no chemical pollution and ________________ (require) no “ airborne vehicles such as aircraft or rockets ”, Wang ________________ (say). “ And there ________________ (be) the possibility of remote control with low cost. ” \n",
      "\n",
      " The experiment ________________ (be) likely to add fuel to the long - running debate in China on the feasibility and environmental impact of large - scale weather modification programmes. \n",
      "\n",
      " Critics ________________ (accuse) Wang, who ________________ (propose) the controversial Sky River project to increase rainfall across Tibet by intercepting wet air circulating over the plateau, of wasting taxpayers ’ money. Others ________________ (say) that even if the sound stimulation method ________________ (work), it would create noise pollution for the people and animals that ________________ (live) in the area. \n",
      "\n",
      " In the study, the rainfall ________________ (be) 11 to 17 per cent higher in areas within the device ’s effective range – a radius of about 500 metres from the sound generator – than outside it. \n",
      "\n",
      " Despite the findings, a researcher from the Institute of Atmospheric Physics under the Chinese Academy of Sciences in Beijing who ________________ (ask) not to be named ________________ (say) Wang ’s two - hour experiment would have to be replicated many times to gather more data. \n",
      "\n",
      " While there ________________ long ________________ (be) speculation that rainfall might be linked to sound – many civilisations ________________ (perform) rain dances in times of drought – the person ________________ (say) there ________________ (be) no physical theories to support the idea. \n",
      "\n",
      " “ The subject ________________ (remain) more of myth than science, ” he ________________ (say).\n"
     ]
    }
   ],
   "source": [
    "processed_text = tense_exercise(text, present_perfect=True)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97267a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy_matches(matches):\n",
    "    \n",
    "    intervals = [[match[1],match[2]] for match in matches]\n",
    "    \n",
    "    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n",
    "    merged = []\n",
    "\n",
    "    for higher in sorted_by_lower_bound:\n",
    "        if not merged:\n",
    "            merged.append(higher)\n",
    "        else:\n",
    "            lower = merged[-1]\n",
    "            # test for intersection between lower and higher:\n",
    "            # we know via sorting that lower[0] <= higher[0]\n",
    "            if higher[0] <= lower[1]:\n",
    "                upper_bound = max(lower[1], higher[1])\n",
    "                merged[-1] = [lower[0], upper_bound]  # replace by merged interval\n",
    "            else:\n",
    "                merged.append(higher)\n",
    "    indices = []\n",
    "    errors = 0\n",
    "    error_merges = []\n",
    "    \n",
    "    for merge in merged:\n",
    "        try:\n",
    "            indices.append(intervals.index(merge))\n",
    "        except:\n",
    "            errors += 1\n",
    "            error_merges.append(merge)\n",
    "            \n",
    "#     print(str(errors) + \" errors occured. The erroroneous merge is \" + str(error_merges))\n",
    "    \n",
    "    new_matches = [matches[index] for index in indices]\n",
    "    \n",
    "    return new_matches\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b48d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "Tenses: \n",
    "Present: VBP/VBZ\n",
    "Present continuous: am/is/are/'m/'s/'re + (RB) + VG\n",
    "Past: VBD\n",
    "Past continuous: was/were + (RB) + VG\n",
    "Future: will/wo/shall + (RB) + VB\n",
    "Future II: am/is/are/'m/'s/'re + (RB) + going + to + VB\n",
    "Future continuous: will/wo/'ll + (RB) + be + (RB) + VG\n",
    "Present perfect: have/has/'ve' + (RB) + VBN\n",
    "Present perfect continuous: have/has + (RB) + been + (RB) + VG\n",
    "Past perfect: had + (RB) + VBN\n",
    "Past perfect continuous: had + (RB) + been + (RB) + VG\n",
    "Future perfect: will/wo/'ll + (RB) + have + (RB) + VBN\n",
    "Future perfect continuous: will/wo + (RB) + been + (RB) + VG\n",
    "Modal verbs: can/could/may/might/should/must/would/'d + (RB) + VB + (RB) + V(any)\n",
    "(Actually include future tenses)\n",
    "(*need to handle tenses myself)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f36371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "def tense_exercise(text,present_tense=True,past_tense=True,future_tense=True,\n",
    "                  present_continuous=True,past_continuous=True,future_continuous=False,\n",
    "                  present_perfect=True,past_perfect=False,future_perfect=False,\n",
    "                  present_perfect_continuous=False,past_perfect_continuous=False,\n",
    "                  future_perfect_continuous=False):\n",
    "    \n",
    "    tense_bool_dict = {\"present_tense\":present_tense,\n",
    "                       \"past_tense\":past_tense,\n",
    "                       \"future_tense\":future_tense,\n",
    "                       \"present_continuous\":present_continuous,\n",
    "                       \"past_continuous\":past_continuous,\n",
    "                       \"future_continuous\":future_continuous,\n",
    "                       \"present_perfect\":present_perfect, \n",
    "                       \"past_perfect\":past_perfect,\n",
    "                       \"future_perfect\":future_perfect,\n",
    "                       \"present_perfect_continuous\":present_perfect_continuous,\n",
    "                       \"past_perfect_continuous\":past_perfect_continuous,\n",
    "                       \"future_perfect_continuous\":future_perfect_continuous}\n",
    "    \n",
    "# Remove parentheses and the contents contained in them\n",
    "    text = re.sub(r'\\([^)]*\\)', '',text)\n",
    "    nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    tenses_priority = [\"future_perfect_continuous\",\"past_perfect_continuous\",\n",
    "                      \"present_perfect_continuous\", \"future_perfect\", \"past_perfect\",\n",
    "                      \"present_perfect\", \"future_continuous\", \"past_continuous\",\n",
    "                      \"present_continuous\", \"future_tense\", \"past_tense\", \"present_tense\", '']\n",
    "\n",
    "    not_list = [\"not\", \"n\\'t\"]\n",
    "    is_list = [\"is\",\"am\",\"are\",\"\\'s\",\"\\'re\",\"ain\\'t\"]\n",
    "    is_list_q = [\"is\", \"am\", \"are\"]\n",
    "    do_list = [\"do\", \"does\"]\n",
    "    was_list = [\"was\",\"were\"]\n",
    "    will_list = [\"will\",\"wo\",\"shall\"]\n",
    "    have_list = [\"have\",\"\\'ve\",\"has\",\"\\'s\"]\n",
    "    adv_dict = {\"TAG\":\"RB\",\"OP\":\"*\"}\n",
    "    noun_dict = {\"TAG\":{\"IN\":[\"PRP\",\"NN\",\"NNP\"]}}\n",
    "\n",
    "    blank = '_'*16\n",
    "\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    present_tense_pattern = [[{\"TAG\":{\"IN\":[\"VBP\",\"VBZ\"]}},adv_dict,{\"TAG\":\"VB\", \"OP\":\"*\"}],\n",
    "                    [{\"LOWER\":{\"IN\":do_list}},adv_dict,noun_dict,adv_dict,{\"TAG\":\"VB\"}],\n",
    "                    [{\"LOWER\":{\"IN\":is_list_q}},adv_dict,noun_dict]]\n",
    "\n",
    "    past_tense_pattern = [[{\"LOWER\":\"did\"},adv_dict,{\"TAG\":\"VB\"}],\n",
    "                        [{\"TAG\":\"VBD\"}],\n",
    "                         [{\"LOWER\":{\"IN\":was_list+[\"did\"]}},adv_dict,{\"TAG\":\"PRP\"},adv_dict]]\n",
    "    future_tense_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,{\"TAG\":\"VB\"}]]\n",
    "    present_cont_pattern = [[{\"LOWER\":{\"IN\":is_list}},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "    past_cont_pattern = [[{\"LOWER\":{\"IN\":was_list}},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "    future_cont_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,\n",
    "                           {\"LOWER\":\"be\"},adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "    present_perfect_pattern = [[{\"LOWER\":{\"IN\":have_list}},\n",
    "                                adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "    past_perfect_pattern = [[{\"LOWER\":\"had\"},adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "    future_perfect_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,\n",
    "                              {\"LOWER\":{\"IN\":have_list}},adv_dict,{\"TAG\":\"VBN\"}]]\n",
    "    present_perfect_cont_pattern = [[{\"LOWER\":{\"IN\":have_list}},adv_dict,\n",
    "                                     {\"LOWER\":\"been\"},adv_dict,\n",
    "                                     {\"TAG\":\"VBG\"}]]\n",
    "    past_perfect_cont_pattern = [[{\"LOWER\":\"had\"},adv_dict,{\"LOWER\":\"been\"},\n",
    "                                  adv_dict,{\"TAG\":\"VBG\"}]]\n",
    "    future_perfect_cont_pattern = [[{\"LOWER\":{\"IN\":will_list}},adv_dict,{\"LOWER\":\"have\"},\n",
    "                                    adv_dict,{\"LOWER\":\"been\"},adv_dict,\n",
    "                                    {\"TAG\":\"VBG\"}]]\n",
    "\n",
    "\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matcher.add(\"present_tense\", present_tense_pattern, greedy='LONGEST')\n",
    "    # matcher.add(\"present_tense_question\", present_tense_q)\n",
    "    matcher.add(\"past_tense\", past_tense_pattern, greedy='LONGEST')\n",
    "    matcher.add(\"future_tense\", future_tense_pattern)\n",
    "    matcher.add(\"present_continuous\", present_cont_pattern)\n",
    "    matcher.add(\"past_continuous\", past_cont_pattern)\n",
    "    matcher.add(\"future_continuous\", future_cont_pattern)\n",
    "    matcher.add(\"present_perfect\", present_perfect_pattern)\n",
    "    matcher.add(\"past_perfect\", past_perfect_pattern)\n",
    "    matcher.add(\"future_perfect\", future_perfect_pattern)\n",
    "    matcher.add(\"present_perfect_continuous\", present_perfect_cont_pattern)\n",
    "    matcher.add(\"past_perfect_continuous\", past_perfect_cont_pattern)\n",
    "    matcher.add(\"future_perfect_continuous\", future_perfect_cont_pattern)\n",
    "\n",
    "    # replace_word(text, \"__________\")\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key=lambda x:x[1])\n",
    "\n",
    "    # print(\"Original matches:\")\n",
    "    # for match_id, start, end in matches:\n",
    "    #     string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    #     span = doc[start:end]  # The matched span\n",
    "    #     start_char, end_char = span.start_char, span.end_char\n",
    "    #     print(string_id, start, end, start_char, end_char, span.text)\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    matches = get_greedy_matches(matches)\n",
    "    matches_copy = matches\n",
    "    \n",
    "    # print(\"New matches:\")\n",
    "    underline_i = []\n",
    "    add_bracket = []\n",
    "    \n",
    "    print(\"Number of blanks: \", len(matches))\n",
    "    \n",
    "    for idx, (match_id, start, end) in enumerate(matches):\n",
    "        if not tense_bool_dict[nlp.vocab.strings[match_id]]:\n",
    "            matches_copy.remove(matches[idx])\n",
    "    matches = matches_copy\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        lemma = ''\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        print(string_id, start, end, span.text)\n",
    "        with_not = False\n",
    "        for idx,token in enumerate(span):\n",
    "            verbs = []\n",
    "            if str(token) in not_list:\n",
    "                with_not = True\n",
    "            if token.pos_ != 'AUX' and token.pos_ != 'PART' and token.pos_ != 'VERB':\n",
    "                pass\n",
    "    #             print(\"Token:\", token)\n",
    "            elif token.pos_ == 'VERB':\n",
    "                verbs.append(token)\n",
    "                lemma = token.lemma_\n",
    "#                 print(token, lemma)\n",
    "#                 print(span)\n",
    "    #             print(\"Lemma:\", lemma)\n",
    "                underline_i.append(token.i)\n",
    "            else:\n",
    "                underline_i.append(token.i)\n",
    "        if not verbs:\n",
    "            lemma = 'be'\n",
    "    #         print(\"Lemma:\", 'be')\n",
    "            underline_i.append(token.i)\n",
    "        if idx == len(span)-1:\n",
    "            if with_not:\n",
    "                add_bracket.append((token.i,'not ' + str(lemma)))\n",
    "            else:\n",
    "                add_bracket.append((token.i,str(lemma)))\n",
    "\n",
    "    underline_i = sorted(list(set(underline_i)))\n",
    "    # print(underline_i)\n",
    "    # print(add_bracket)\n",
    "    bracket_idx_list, lemma_list = list(list(zip(*add_bracket))[0]), list(list(zip(*add_bracket))[1])\n",
    "\n",
    "    tokenized = [str(token) for token in doc]\n",
    "    # print(tokenized)\n",
    "    for i,word in enumerate(tokenized):\n",
    "        if i in bracket_idx_list:\n",
    "            tokenized[i] = blank + '(' + lemma_list[bracket_idx_list.index(i)] + ')'\n",
    "        elif i in underline_i:\n",
    "            tokenized[i] = blank\n",
    "            \n",
    "    processed_text = untokenize(tokenized)\n",
    "\n",
    "#     processed_text = TreebankWordDetokenizer().detokenize(tokenized)\n",
    "\n",
    "    # Combine the neighboring blanks\n",
    "    processed_text = re.sub(r'(' + blank + ' |' + blank + ')+', blank+' ', processed_text)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1320b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Span object:\n",
      "\n",
      "class Span(builtins.object)\n",
      " |  A slice from a Doc object.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/span\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      Get a `Token` or a `Span` object\n",
      " |      \n",
      " |      i (int or tuple): The index of the token within the span, or slice of\n",
      " |          the span to get.\n",
      " |      RETURNS (Token or Span): The token at `span[i]`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#getitem\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      Iterate over `Token` objects.\n",
      " |      \n",
      " |      YIELDS (Token): A `Token` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#iter\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      Get the number of tokens in the span.\n",
      " |      \n",
      " |      RETURNS (int): The number of tokens in the span.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#len\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Span.__reduce__(self)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_doc(...)\n",
      " |      Span.as_doc(self, *, bool copy_user_data=False)\n",
      " |      Create a `Doc` object with a copy of the `Span`'s data.\n",
      " |      \n",
      " |              copy_user_data (bool): Whether or not to copy the original doc's user data.\n",
      " |              RETURNS (Doc): The `Doc` copy of the span.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#as_doc\n",
      " |  \n",
      " |  char_span(...)\n",
      " |      Span.char_span(self, int start_idx, int end_idx, label=0, kb_id=0, vector=None)\n",
      " |      Create a `Span` object from the slice `span.text[start : end]`.\n",
      " |      \n",
      " |              start (int): The index of the first character of the span.\n",
      " |              end (int): The index of the first character after the span.\n",
      " |              label (uint64 or string): A label to attach to the Span, e.g. for\n",
      " |                  named entities.\n",
      " |              kb_id (uint64 or string):  An ID from a KB to capture the meaning of a named entity.\n",
      " |              vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
      " |                  the span.\n",
      " |              RETURNS (Span): The newly constructed object.\n",
      " |  \n",
      " |  get_lca_matrix(...)\n",
      " |      Span.get_lca_matrix(self)\n",
      " |      Calculates a matrix of Lowest Common Ancestors (LCA) for a given\n",
      " |              `Span`, where LCA[i, j] is the index of the lowest common ancestor among\n",
      " |              the tokens span[i] and span[j]. If they have no common ancestor within\n",
      " |              the span, LCA[i, j] will be -1.\n",
      " |      \n",
      " |              RETURNS (np.array[ndim=2, dtype=numpy.int32]): LCA matrix with shape\n",
      " |                  (n, n), where n = len(self).\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#get_lca_matrix\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Span.similarity(self, other)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |              similarity using an average of word vectors.\n",
      " |      \n",
      " |              other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |                  `Span`, `Token` and `Lexeme` objects.\n",
      " |              RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#similarity\n",
      " |  \n",
      " |  to_array(...)\n",
      " |      Span.to_array(self, py_attr_ids) -> ndarray\n",
      " |      Given a list of M attribute IDs, export the tokens to a numpy\n",
      " |              `ndarray` of shape `(N, M)`, where `N` is the length of the document.\n",
      " |              The values will be 32-bit integers.\n",
      " |      \n",
      " |              attr_ids (list[int]): A list of attribute ID ints.\n",
      " |              RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
      " |                  per word, and one column per attribute indicated in the input\n",
      " |                  `attr_ids`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Span.get_extension(type cls, name)\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Span.has_extension(type cls, name)\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#has_extension\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Span.remove_extension(type cls, name)\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |                  removed extension.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Span.set_extension(type cls, name, **kwargs)\n",
      " |      Define a custom attribute which becomes available as `Span._`.\n",
      " |      \n",
      " |              name (str): Name of the attribute to set.\n",
      " |              default: Optional default value of the attribute.\n",
      " |              getter (callable): Optional getter function.\n",
      " |              setter (callable): Optional setter function.\n",
      " |              method (callable): Optional method for method extension.\n",
      " |              force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/span#set_extension\n",
      " |              USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  conjuncts\n",
      " |      Tokens that are conjoined to the span's root.\n",
      " |      \n",
      " |      RETURNS (tuple): A tuple of Token objects.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#lefts\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  end\n",
      " |  \n",
      " |  end_char\n",
      " |  \n",
      " |  ent_id\n",
      " |      RETURNS (uint64): The entity ID.\n",
      " |  \n",
      " |  ent_id_\n",
      " |      RETURNS (str): The (string) entity ID.\n",
      " |  \n",
      " |  ents\n",
      " |      The named entities in the span. Returns a tuple of named entity\n",
      " |      `Span` objects, if the entity recognizer has been applied.\n",
      " |      \n",
      " |      RETURNS (tuple): Entities in the span, one `Span` per entity.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#ents\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#has_vector\n",
      " |  \n",
      " |  kb_id\n",
      " |  \n",
      " |  kb_id_\n",
      " |      RETURNS (str): The named entity's KB ID.\n",
      " |  \n",
      " |  label\n",
      " |  \n",
      " |  label_\n",
      " |      RETURNS (str): The span's label.\n",
      " |  \n",
      " |  lefts\n",
      " |      Tokens that are to the left of the span, whose head is within the\n",
      " |      `Span`.\n",
      " |      \n",
      " |      YIELDS (Token):A left-child of a token of the span.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#lefts\n",
      " |  \n",
      " |  lemma_\n",
      " |      RETURNS (str): The span's lemma.\n",
      " |  \n",
      " |  n_lefts\n",
      " |      The number of tokens that are to the left of the span, whose\n",
      " |      heads are within the span.\n",
      " |      \n",
      " |      RETURNS (int): The number of leftward immediate children of the\n",
      " |          span, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#n_lefts\n",
      " |  \n",
      " |  n_rights\n",
      " |      The number of tokens that are to the right of the span, whose\n",
      " |      heads are within the span.\n",
      " |      \n",
      " |      RETURNS (int): The number of rightward immediate children of the\n",
      " |          span, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#n_rights\n",
      " |  \n",
      " |  noun_chunks\n",
      " |      Iterate over the base noun phrases in the span. Yields base\n",
      " |      noun-phrase #[code Span] objects, if the language has a noun chunk iterator.\n",
      " |      Raises a NotImplementedError otherwise.\n",
      " |      \n",
      " |      A base noun phrase, or \"NP chunk\", is a noun\n",
      " |      phrase that does not permit other NPs to be nested within it – so no\n",
      " |      NP-level coordination, no prepositional phrases, and no relative\n",
      " |      clauses.\n",
      " |      \n",
      " |      YIELDS (Span): Noun chunks in the span.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#noun_chunks\n",
      " |  \n",
      " |  orth_\n",
      " |      Verbatim text content (identical to `Span.text`). Exists mostly for\n",
      " |      consistency with other attributes.\n",
      " |      \n",
      " |      RETURNS (str): The span's text.\n",
      " |  \n",
      " |  rights\n",
      " |      Tokens that are to the right of the Span, whose head is within the\n",
      " |      `Span`.\n",
      " |      \n",
      " |      YIELDS (Token): A right-child of a token of the span.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#rights\n",
      " |  \n",
      " |  root\n",
      " |      The token with the shortest path to the root of the\n",
      " |      sentence (or the root itself). If multiple tokens are equally\n",
      " |      high in the tree, the first token is taken.\n",
      " |      \n",
      " |      RETURNS (Token): The root token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#root\n",
      " |  \n",
      " |  sent\n",
      " |      Obtain the sentence that contains this span. If the given span\n",
      " |      crosses sentence boundaries, return only the first sentence\n",
      " |      to which it belongs.\n",
      " |      \n",
      " |      RETURNS (Span): The sentence span that the span is a part of.\n",
      " |  \n",
      " |  sentiment\n",
      " |      RETURNS (float): A scalar value indicating the positivity or\n",
      " |      negativity of the span.\n",
      " |  \n",
      " |  start\n",
      " |  \n",
      " |  start_char\n",
      " |  \n",
      " |  subtree\n",
      " |      Tokens within the span and tokens which descend from them.\n",
      " |      \n",
      " |      YIELDS (Token): A token within the span, or a descendant from it.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#subtree\n",
      " |  \n",
      " |  tensor\n",
      " |      The span's slice of the doc's tensor.\n",
      " |      \n",
      " |      RETURNS (ndarray[ndim=2, dtype='float32']): A 2D numpy or cupy array\n",
      " |          representing the span's semantics.\n",
      " |  \n",
      " |  text\n",
      " |      RETURNS (str): The original verbatim text of the span.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      The text content of the span with a trailing whitespace character if\n",
      " |      the last token has one.\n",
      " |      \n",
      " |      RETURNS (str): The text content of the span (with trailing\n",
      " |          whitespace).\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation. Defaults to an average of the\n",
      " |      token vectors.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the span's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the span's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/span#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |      RETURNS (Vocab): The Span's Doc's vocab.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = [[{\"TAG\":{\"IN\":['VBZ','VBP']}},adv_dict,{\"POS\":\"VERB\",\"OP\":\"!\"}]]\n",
    "\n",
    "text = \"I have not done yet.\"\n",
    "doc = nlp(text)\n",
    "matcher2 = Matcher(nlp.vocab)\n",
    "matcher2.add(\"pattern\",pattern)\n",
    "# help(matcher2)\n",
    "matches = matcher2(doc, as_spans=True)\n",
    "help(matches[0])\n",
    "# for match_id, start, end in matches:\n",
    "#     string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "#     span = doc[start:end]  # The matched span\n",
    "#     print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e3f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON      PRP       nsubj     I         \n",
      "Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "take        VERB      VBP       ROOT      take      \n",
      "Tense=Pres|VerbForm=Fin\n",
      "part        NOUN      NN        dobj      part      \n",
      "Number=Sing\n",
      "in          ADP       IN        prep      in        \n",
      "\n",
      "it          PRON      PRP       pobj      it        \n",
      "Case=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      ".           PUNCT     .         punct     .         \n",
      "PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I take part in it.\")\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_tag = token.tag_\n",
    "    token_dep = token.dep_\n",
    "    token_morph = token.morph\n",
    "    token_lemma = token.lemma_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_tag:<10}{token_dep:<10}{token_lemma:<10}\")\n",
    "    print(token_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce0bd370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'particle'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PART\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
